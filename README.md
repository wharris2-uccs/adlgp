# Advanced Deep Learning Graphics Pipeline (ADLGP)

This repository hosts source code related to our work on the "Advanced Deep Learning Graphics Pipeline", a proposed pipeline for improving rendering speed with intermediate deep learning stages. Our solution is not yet fully implemented, but has been evaluated as a proof of concept and published as a Technical Communications paper in SIGGRAPH Asia 2021 (DOI: [https://doi.org/10.1145/3478512.3488609](https://doi.org/10.1145/3478512.3488609)).

Our work could not have been possible without the groundbreaking contributions in deep learning image generation. The two architectures we used in our paper are the AttnGAN ([repository](https://github.com/taoxugit/AttnGAN), [paper](https://arxiv.org/abs/1711.10485)) and SRGAN ([repository](https://github.com/aitorzip/PyTorch-SRGAN), [paper](https://arxiv.org/abs/1609.04802)).

## Proposed ADLGP Architecture
Our proposed framework converts 3D scene data in the form of text into a 
corresponding image representing a given view of the scene, e.g. a rendering.

<img width="670" alt="process_diagram" src="https://user-images.githubusercontent.com/13023201/136856967-f690d581-a322-4fc2-8bb6-718dfe86d2d4.png">

## Process
<img width="670" alt="flow_diagram_remake_tall" src="https://user-images.githubusercontent.com/13023201/136858020-1501eb19-5b2c-47d6-a391-038b7c2daf8d.PNG">

We first pre-processed the 3D scene to generate attribute-image pairs.
We found smaller images, from 4 x 4 to 16 x 16 pixels worked the best. Each input frame was broken into these 4 x 4 pixel "frameblocks" and then processed in the scene to obtain semantic attributes. We identified where in a frameblock an object resided using subdivision and clipping, as well as what part of the object was shown by grouping visible faces together. Other attributes we included were object name, depth from the camera, and concatenated rotation vector. Here we show a sample frameblock and it's attributes.

<img width="670" alt="attributes_bunny_pair" src="https://user-images.githubusercontent.com/13023201/136860054-230a4978-d71d-4767-89c8-5a4e12e26a83.png">

We trained the AttnGAN architecture to generate a 4 x 4 pixel image corresponding to the aforementioned text attributes.
To create an intermediate frame from AttnGAN, attributes for each frameblock in the frame were used to generate rough images which were then placed into their corresponding block positions inside the frame. This gave us the structural base used to train and test the SRGAN model.

Because the SRGAN is trained on intermediate outputs, all known training frames must be generated and trained on to completely initialize the framework. Any frame generated by AttnGAN can then be fed into the initialized SRGAN model to output the final refined image. This process was completed for the sequence of 10 frames we evaluated, half of which were known to the training dataset.

## Running
**Disclaimer** &mdash; herein is described an overview of our process for training and generating our framework. This is extremely nuanced and specific to our setup, and thus we suggest you create or adjust the pipeline to fit your needs. Be warned, it may be difficult to get portions of the code to run on your system.

#### Generate Attributes from Scene
Initialize the Autodesk Maya scene with test objects and load the ```maya/generate_semantics.py``` script. This script exports attributes from the scene from what is visible in the camera viewport (i.e. what will be rendered for each frame). Before running, you will need to ensure the viewport window exactly matches the output ratio, as shown below. Run the script, which will open a window for user input. Please check the hardcoded output files paths, if any, and replace with your custom directory setup. After running, please check the output folder, which should contain numbered folders of all extracted text semantics for each frame processed.

#### Generate Frameblocks from Frames
You must render data frames before training. Process these frames from ```python/datasets/generate_frameblocks.py``` script, run from the command line. The script will not only output frameblocks, but also skip similar blocks from the same area, which greatly improves recognition. Please see the script for more details on input and output directories. These can be passed in as arguments or hard coded for your convenience.

#### Homogenize Dataset
Now that all attributes have been generated, and all frameblocks selected from the frame, you must export the image-text pairs to form the inputs of the AttnGAN architecture. Homogenize the dataset into these pairs using ```python/datasets/homogenize_frameblocks.py```. Please see the script for more details on input and output directories. The final outputs should be placed inside the AttnGAN ```data/<name>/images/``` directory under an appropriately labled folder (i.e. ```data/bunny/images/```).

#### Train AttnGAN
Follow the instructions from the [source repo](https://github.com/taoxugit/AttnGAN) to download and train the AttnGAN model.

Once the model has been trained, you must export all frameblocks for the set of frames you wish to generate. These may be known or unknown ("novel") frames. To do this, run the ```maya/generate_semantics.py``` script from Autodesk Maya with test flags set. Please see the input window and script for more details. These will be exported inside of the ```python/attngan/data/<name>/``` folder, named "eval_#", where "#" is the frame exported. These eval folders will contain all frameblocks for the frame. If using the frameblock size of 4, for example, there will be 32,400 exported files in the folder.

Also contained in the directory must be a file named ```example_filenames_#.txt```, with "#" corresponding to each frame. This is a list of the captions to generate from, and is simply an enumeration of each ```eval_#/``` folder. These files will be created by the PyMel script.

#### Generate SRGAN Training Set
Now you are ready to generate the SRGAN training set. First, select the frame to be generated by renaming the ```example_filenames_#.txt``` file to ```example_filenames.txt```. Then run the AttnGAN model in eval mode, following the instructions from the [source repo](https://github.com/taoxugit/AttnGAN).
Output will be images for all generators trained in the AttnGAN.

Organize these into complete images using the scripts inside of ```python/dataset/```. 

#### Train SRGAN
Follow the instructions from the [source repo](https://github.com/aitorzip/PyTorch-SRGAN) to download and train the SRGAN model. As with the AttnGAN, the scripts inside of ```python/dataset/``` may be helpful to you for homogenizing the dataset. To train our SRGAN, we used only the generated completed frames from AttnGAN, and the real target frames. The frameblocks output in previous stages were not used, and in fact rendered poorer images when run through SRGAN.

SRGAN will output the final fake images, which we show in the figures above. Congratulations, you have successfully converted scene semantics into a rendered frame using the ADLGP machine learning framework!

## Evaluation
Please see the ```python/utils/``` directory, and specifically the ```utils.py``` file, for methods to evaluate and plot the results of the framework. Our training scripts included CSV exports to track this data, if you built the architectures from source we recommend you copy those lines into the appropriate training scripts.

## Reference
* DOI: [https://doi.org/10.1145/3478512.3488609](https://doi.org/10.1145/3478512.3488609)
* AttnGAN ([repository](https://github.com/taoxugit/AttnGAN), [paper](https://arxiv.org/abs/1711.10485))
* SRGAN ([repository](https://github.com/aitorzip/PyTorch-SRGAN), [paper](https://arxiv.org/abs/1609.04802))
